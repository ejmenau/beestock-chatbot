# app.py
import streamlit as st
import json
import faiss
import numpy as np
from sentence_transformers import SentenceTransformer
import google.generativeai as genai
import os

# Configure Gemini API
genai.configure(api_key="AIzaSyApjah1pIDAbpG7O2guSi1UcqFKwrEo7hs")

@st.cache_resource
def load_rag_system():
    """Carrega o sistema RAG com tratamento de erros para arquivos ausentes."""
    
    # Verifica se os arquivos necess√°rios existem
    required_files = ['vector_index.faiss', 'chunks_data.json']
    missing_files = []
    
    for file in required_files:
        if not os.path.exists(file):
            missing_files.append(file)
    
    if missing_files:
        st.error(f"‚ùå Arquivos necess√°rios n√£o encontrados: {', '.join(missing_files)}")
        st.info("""
        **Para resolver este problema:**
        
        1. **Execute o script de indexa√ß√£o localmente primeiro:**
           ```bash
           python build_index.py
           ```
        
        2. **Certifique-se de que estes arquivos foram gerados:**
           - `chunks_data.json`
           - `vector_index.faiss`
        
        3. **Inclua-os no seu deployment ou execute a indexa√ß√£o no servidor.**
        """)
        return None, None, None
    
    try:
        # Carrega o √≠ndice vetorial FAISS
        index = faiss.read_index('vector_index.faiss')
        
        # Carrega os dados dos chunks
        with open('chunks_data.json', 'r', encoding='utf-8') as f:
            chunks_data = json.load(f)
        
        # Carrega o modelo de embedding
        embedding_model = SentenceTransformer('distiluse-base-multilingual-cased-v2')
        
        return index, chunks_data, embedding_model
        
    except Exception as e:
        st.error(f"‚ùå Erro ao carregar o sistema RAG: {str(e)}")
        return None, None, None

def search_knowledge_base(query, index, chunks_data, embedding_model, k=5, source_type_filter=None):
    """Busca na base de conhecimento por chunks relevantes."""
    try:
        # Gera embedding para a consulta
        query_embedding = embedding_model.encode([query])
        
        # Normaliza o embedding da consulta para similaridade de cosseno
        faiss.normalize_L2(query_embedding)
        
        # Busca no √≠ndice FAISS
        search_k = min(k * 3, len(chunks_data))
        distances, indices = index.search(query_embedding, search_k)
        
        # Recupera os chunks correspondentes
        relevant_chunks = []
        for idx in indices[0]:
            if idx < len(chunks_data):
                chunk = chunks_data[idx]
                
                # Aplica filtro de tipo de fonte se especificado
                if source_type_filter:
                    if chunk.get('source_type') != source_type_filter:
                        continue
                
                relevant_chunks.append(chunk)
                
                if len(relevant_chunks) >= k:
                    break
        
        return relevant_chunks
        
    except Exception as e:
        st.error(f"Erro na busca: {str(e)}")
        return []

def generate_answer(query, relevant_chunks):
    """Gera uma resposta usando Gemini baseada no contexto recuperado."""
    try:
        if not relevant_chunks:
            return "N√£o consegui encontrar informa√ß√µes relevantes na base de conhecimento para responder sua pergunta."
        
        # Combina os chunks recuperados em contexto
        context_str = "\n\n---\n".join([chunk['text'] for chunk in relevant_chunks])
        
        # Cria template de prompt em portugu√™s
        prompt_template = f"""Voc√™ √© um analista de suporte especialista no sistema WMS BeeStock (Sistema de Gerenciamento de Armaz√©m). 
Seu papel √© ajudar os usu√°rios a entender processos, procedimentos e informa√ß√µes relacionadas ao BeeStock.

IMPORTANTE: Responda √† pergunta do usu√°rio baseado APENAS nas informa√ß√µes fornecidas no contexto abaixo. 
N√£o use conhecimento externo ou fa√ßa suposi√ß√µes al√©m do que est√° declarado no contexto.
Se o contexto n√£o contiver informa√ß√µes suficientes para responder completamente √† pergunta, diga isso claramente.

Informa√ß√µes do contexto:
{context_str}

Pergunta do usu√°rio: {query}

Por favor, forne√ßa uma resposta clara e √∫til baseada no contexto acima:"""

        # Gera resposta usando Gemini Pro
        model = genai.GenerativeModel('gemini-pro')
        response = model.generate_content(prompt_template)
        
        return response.text.strip()
        
    except Exception as e:
        return f"Desculpe, encontrei um erro ao gerar a resposta: {str(e)}."

def format_sources(chunks):
    """Formata informa√ß√µes das fontes para exibi√ß√£o."""
    if not chunks:
        return ""
    
    source_groups = {}
    
    for chunk in chunks:
        source_type = chunk.get('source_type', 'unknown')
        if source_type not in source_groups:
            source_groups[source_type] = []
        
        source_info = {
            'filename': chunk.get('source_filename', 'Desconhecido'),
            'customer': chunk.get('customer_name', 'Desconhecido')
        }
        
        if chunk.get('title'):
            source_info['title'] = chunk['title']
        
        source_groups[source_type].append(source_info)
    
    # Formata as fontes
    sources_text = "\n\n---\n**Fontes Utilizadas:**\n"
    
    for source_type, sources in source_groups.items():
        if source_type == 'wiki_documentation':
            sources_text += "\nüìö **Documenta√ß√£o Wiki:**\n"
            for source in sources:
                title = source.get('title', source['filename'])
                sources_text += f"  - {title}\n"
        else:
            sources_text += f"\nüí¨ **Transcri√ß√µes de Cliente ({sources[0]['customer']}):**\n"
            for source in sources:
                sources_text += f"  - {source['filename']}\n"
    
    return sources_text

# Interface Streamlit
st.set_page_config(
    page_title="Chatbot RAG BeeStock WMS",
    page_icon="ü§ñ",
    layout="wide"
)

st.title("ü§ñ Chatbot RAG BeeStock WMS")
st.markdown("Fa√ßa perguntas sobre processos do BeeStock WMS baseado em nossa base de conhecimento.")

# Carrega o sistema RAG
with st.spinner("Carregando sistema RAG..."):
    index, chunks_data, embedding_model = load_rag_system()

if index is None or chunks_data is None or embedding_model is None:
    st.stop()

# Mostra informa√ß√µes da base de conhecimento
with st.expander("üìä Informa√ß√µes da Base de Conhecimento"):
    source_types = {}
    customer_names = set()
    
    for chunk in chunks_data:
        source_type = chunk.get('source_type', 'unknown')
        source_types[source_type] = source_types.get(source_type, 0) + 1
        
        if chunk.get('customer_name') != 'Documenta√ß√£o Wiki':
            customer_names.add(chunk.get('customer_name', 'Desconhecido'))
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.write("**Tipos de Fonte:**")
        for source_type, count in source_types.items():
            if source_type == 'wiki_documentation':
                st.write(f"- Documenta√ß√£o Wiki: {count} chunks")
            elif source_type == 'customer_transcript':
                st.write(f"- Transcri√ß√µes de Cliente: {count} chunks")
            else:
                st.write(f"- {source_type}: {count} chunks")
    
    with col2:
        if customer_names:
            st.write("**Clientes:**")
            for customer in sorted(customer_names):
                st.write(f"- {customer}")

# Interface de busca
st.markdown("---")

# Entrada da consulta
query = st.text_input("üí¨ Fa√ßa sua pergunta:", placeholder="ex: Como consultar EAN?")

# Op√ß√µes de busca
col1, col2 = st.columns([2, 1])

with col1:
    source_filter = st.selectbox(
        "üîç Filtrar por tipo de fonte:",
        ["Todas as fontes", "Documenta√ß√£o Wiki", "Transcri√ß√µes de Cliente"],
        help="Escolha qual tipo de documento pesquisar"
    )

with col2:
    k_results = st.slider("N√∫mero de resultados:", min_value=1, max_value=10, value=5)

# Converte filtro para formato interno
source_type_filter = None
if source_filter == "Documenta√ß√£o Wiki":
    source_type_filter = "wiki_documentation"
elif source_filter == "Transcri√ß√µes de Cliente":
    source_type_filter = "customer_transcript"

# Bot√£o de busca
if st.button("üîç Buscar na Base de Conhecimento", type="primary"):
    if query.strip():
        with st.spinner("Buscando na base de conhecimento..."):
            # Busca por chunks relevantes
            relevant_chunks = search_knowledge_base(
                query, index, chunks_data, embedding_model, 
                k=k_results, source_type_filter=source_type_filter
            )
            
            if relevant_chunks:
                # Gera resposta
                with st.spinner("Gerando resposta..."):
                    answer = generate_answer(query, relevant_chunks)
                
                # Exibe resultados
                st.markdown("---")
                st.markdown("### ü§ñ Resposta")
                st.write(answer)
                
                # Mostra fontes
                sources_info = format_sources(relevant_chunks)
                st.markdown(sources_info)
                
                # Mostra chunks brutos para debug
                with st.expander("üîç Ver Chunks Recuperados"):
                    for i, chunk in enumerate(relevant_chunks):
                        st.markdown(f"**Chunk {i+1}** (de {chunk.get('source_filename', 'Desconhecido')})")
                        st.text(chunk['text'][:500] + "..." if len(chunk['text']) > 500 else chunk['text'])
                        st.markdown("---")
            else:
                st.warning("Nenhuma informa√ß√£o relevante encontrada. Tente reformular sua pergunta ou verificar diferentes tipos de fonte.")
    else:
        st.warning("Por favor, digite uma pergunta para buscar.")

# Rodap√©
st.markdown("---")
st.markdown("""
**üí° Dicas:**
- Tente diferentes formula√ß√µes se n√£o obtiver os resultados esperados
- Use o filtro de tipo de fonte para focar em tipos espec√≠ficos de documento
- Verifique os chunks recuperados para ver exatamente quais informa√ß√µes foram encontradas
""")
