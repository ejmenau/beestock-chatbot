# app.py
import streamlit as st
import json
import faiss
import numpy as np
from sentence_transformers import SentenceTransformer
import google.generativeai as genai
import os

# --- CONFIGURA√á√ÉO DA P√ÅGINA ---
st.set_page_config(
    page_title="KB BeeStock Chatbot",
    page_icon="üêù",
    layout="centered",
    initial_sidebar_state="auto"
)

# --- FUN√á√ÉO DE CACHE PARA CARREGAR O MODELO E OS DADOS ---
# O cache acelera o carregamento, executando esta fun√ß√£o apenas uma vez.
@st.cache_resource
def load_rag_system():
    """
    Carrega todos os componentes necess√°rios para o sistema RAG.
    Isso inclui o √≠ndice FAISS, os dados dos chunks e o modelo de embedding.
    """
    try:
        index = faiss.read_index("vector_index.faiss")
        with open("chunks_data.json", 'r', encoding='utf-8') as f:
            chunks_data = json.load(f)
        # **FIX**: Using the powerful multilingual model
        embedding_model = SentenceTransformer('paraphrase-multilingual-mpnet-base-v2')
        return index, chunks_data, embedding_model
    except FileNotFoundError:
        st.error("Arquivos de √≠ndice (vector_index.faiss ou chunks_data.json) n√£o encontrados. "
                 "Por favor, execute o script `build_index.py` primeiro.")
        return None, None, None

# --- CARREGAMENTO DOS COMPONENTES DO SISTEMA ---
index, chunks_data, embedding_model = load_rag_system()

# --- CONFIGURA√á√ÉO DA API KEY DO GEMINI ---
# Use o segredo do Streamlit para armazenar a chave de API de forma segura
try:
    # Tenta obter a chave da API dos segredos do Streamlit (melhor para deploy)
    api_key = st.secrets["GEMINI_API_KEY"]
except (FileNotFoundError, KeyError):
    # Se n√£o encontrar, pede ao usu√°rio para inserir no sidebar (bom para desenvolvimento local)
    st.sidebar.warning("üîë Chave de API do Gemini n√£o encontrada. Por favor, insira abaixo.")
    api_key = st.sidebar.text_input("Insira sua Chave de API do Gemini:", type="password", help="Obtenha sua chave em [Google AI Studio](https://makersuite.google.com/)")

if api_key:
    try:
        genai.configure(api_key=api_key)
        # **FIX**: Using the latest stable model 'gemini-1.5-flash-latest'
        generative_model = genai.GenerativeModel('gemini-1.5-flash-latest')
    except Exception as e:
        st.error(f"Erro ao configurar a API do Gemini: {e}")
        generative_model = None
else:
    generative_model = None


# --- FUN√á√ïES DO CHATBOT ---
def search_knowledge_base(query, k=5, customer_filter=None):
    """
    Busca na base de conhecimento os chunks mais relevantes para a consulta.
    """
    if embedding_model is None or index is None:
        return []

    query_embedding = embedding_model.encode([query])
    faiss.normalize_L2(query_embedding)

    # A busca no FAISS retorna dist√¢ncias e √≠ndices
    distances, indices = index.search(query_embedding, k * 2) # Busca mais para garantir resultados ap√≥s o filtro

    retrieved_chunks = []
    for i in indices[0]:
        if i != -1:
            chunk_info = chunks_data[i]
            # Aplica o filtro de cliente se fornecido
            if customer_filter:
                if "metadata" in chunk_info and chunk_info["metadata"].get("customer", "").lower() == customer_filter.lower():
                    retrieved_chunks.append(chunk_info)
            else:
                retrieved_chunks.append(chunk_info)
    
    return retrieved_chunks[:k] # Retorna o n√∫mero correto de chunks ap√≥s o filtro

def generate_response(query, retrieved_chunks):
    """
    Gera uma resposta usando o modelo Gemini com base nos chunks recuperados.
    """
    if not retrieved_chunks or generative_model is None:
        return "N√£o foi poss√≠vel encontrar informa√ß√µes relevantes para responder √† sua pergunta."

    context_str = "\n\n---\n\n".join([chunk["text_chunk"] for chunk in retrieved_chunks if "text_chunk" in chunk])

    prompt_template = f"""
    Voc√™ √© um analista de suporte especialista no sistema WMS da BeeStock. Sua tarefa √© responder √† pergunta do usu√°rio de forma clara e objetiva, em portugu√™s do Brasil.
    
    Use SOMENTE o contexto fornecido abaixo para formular sua resposta. N√£o invente informa√ß√µes.
    Se a informa√ß√£o n√£o estiver no contexto, diga que n√£o possui informa√ß√µes suficientes para responder.

    CONTEXTO:
    {context_str}

    PERGUNTA:
    {query}

    RESPOSTA:
    """
    
    try:
        response = generative_model.generate_content(prompt_template)
        return response.text
    except Exception as e:
        return f"Ocorreu um erro ao gerar a resposta: {e}"

# --- INTERFACE DO USU√ÅRIO (UI) ---
st.title("üêù Chatbot da Base de Conhecimento BeeStock")
st.markdown("Fa√ßa uma pergunta sobre os processos dos clientes e o chatbot buscar√° a resposta nos documentos.")

# Sidebar para filtros
st.sidebar.header("Filtros de Busca")
customer_list = sorted(list(set(chunk['metadata']['customer'] for chunk in chunks_data if 'metadata' in chunk and 'customer' in chunk['metadata']))) if chunks_data else []

customer_filter = st.sidebar.selectbox(
    "Filtrar por cliente (opcional):",
    options=["Todos"] + customer_list
)
customer_filter_value = None if customer_filter == "Todos" else customer_filter

# Inicializa o hist√≥rico do chat
if "messages" not in st.session_state:
    st.session_state.messages = []

# Exibe as mensagens do hist√≥rico
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# Campo de entrada do usu√°rio
if prompt := st.chat_input("Qual √© a sua d√∫vida?"):
    # Adiciona a mensagem do usu√°rio ao hist√≥rico
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # Gera e exibe a resposta do assistente
    with st.chat_message("assistant"):
        with st.spinner("Pensando... üß†"):
            if index is not None and generative_model is not None:
                chunks = search_knowledge_base(prompt, customer_filter=customer_filter_value)
                response = generate_response(prompt, chunks)
                st.markdown(response)
                # Adiciona a resposta do assistente ao hist√≥rico
                st.session_state.messages.append({"role": "assistant", "content": response})
            elif index is None:
                 st.error("O sistema de busca n√£o foi carregado. Verifique os arquivos de √≠ndice.")
            else:
                 st.error("A chave de API do Gemini n√£o foi configurada. Por favor, adicione-a no menu lateral.")
